<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Propensity Score Analysis (PSA) – Day 1</title>
    <meta charset="utf-8" />
    <meta name="author" content="Chaochen Wang (CWAN)   Thomas Laurent (TLAU)" />
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Propensity Score Analysis (PSA) – Day 1
### Chaochen Wang (CWAN) <br> Thomas Laurent (TLAU)
### 2019-12-13 17:00~18:00 <span class="citation">@CSS</span>

---



class: middle
# Objectives

- Revision about causal inference framework and terminologies.

- Introduction of Propensity Score Analysis (PSA).  

- Understand the **pre-assumptions** that required for PSA.

- How to test for unobserved confounders? &lt;br&gt;(by TLAU)

- Discuss the ways of using Propensity Scores. 

&lt;!-- - Understand that PSA is **not omnipotent** --&gt;

???


---
class: middle, center, inverse

## Some background before PSA


---
class: middle

## What do we mean by &lt;br&gt;"causal inference"? (1)


- Most data analysis in medical research (or in other area) has a central aim - to learn about **cause-effect relationships**. 

    - Does the treatment work? 
    - How harmful is the exposure? 
    - How effective would the policy be and why?
    
    
---
class: middle

## What do we mean by &lt;br&gt;"causal inference"? (2)

- Randomised studies are **randomised** precisely to make causal inference more reliable. 

- When randomisation is not feasible, we still wish to make inferences about effects of causes. 

- However, most medical studies can only be interpreted as *"associations"* rather than causal effect. 


---
class: middle

## Example


Anaemic patients undergoing hip replacement operation might benefit from receiving an **intravenous iron supplement** prior to surgery. 

- Data collected in one hospital on all anaemic patients undergoing a hip replacement operation between 2009 and 2014. 

- `\(X\)`:  **intravenous iron supplement**, yes or no
- `\(Y\)`: 90 days survival after operation, alive or dead

---
class: middle 

### Other data collected in the example: 

- Age, gender, co-morbidities (CVD, diabetes, renal disease);

- Severity of anaemia;

- Operation types;

- Whether or not transfusion needed during operation;

- Length of stay in hospital;

---
class: middle

## Traditional Approach - crude 

- We start by looking at a 2 `\(\times\)` 2 table:


&lt;table class="table table-striped table-hover table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="border-bottom:hidden" colspan="2"&gt;&lt;/th&gt;
&lt;th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2"&gt;&lt;div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; "&gt;Death90&lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:center;"&gt;  &lt;/th&gt;
   &lt;th style="text-align:center;"&gt;  &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; 0 &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; 1 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;font-weight: bold;vertical-align: middle !important;" rowspan="2"&gt; FeIV &lt;/td&gt;
   &lt;td style="text-align:center;font-weight: bold;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 9206 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 376 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   
   &lt;td style="text-align:center;font-weight: bold;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 7365 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 312 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


- Estimated log odds ratio `\(= \log(\frac{312\times9206}{376\times7365}) = 0.04, 95\% \text{CI: } -0.12, 0.19\)` 

---
class: middle
## Traditional Approach - crude 

- What are we estimating here? The **estimand**&lt;sup&gt;1&lt;/sup&gt; is 

$$
`\begin{aligned}
\log\text{OR}_{Y|X} &amp; = \log{\frac{\text{Pr}(Y = 1 | X = 1)}{1- \text{Pr}(Y = 1 | X = 1)}} \\
                    &amp; \;\;\;\;\;\;\;\;\; - \log{\frac{\text{Pr}(Y = 1 | X = 0)}{1- \text{Pr}(Y = 1 | X = 0)}}
\end{aligned}`
$$

.footnote[
[1] **Estimand** is defined as the value that we we would like to know, such as "the mean BMI of the population in Japan". This is an unknown but fixed quantity.
]

---
class: middle

## Confounding

- We cannot give `\(\log\text{OR}_{Y|X}\)` a causal interpretation because there is **confounding**. 
- It is possible that patients provided with intravenous iron were already worse off, masking a truely effect of the exposure. 
- So the estimand changed into a **conditional logOR**: 


$$
`\begin{aligned}
\log\text{OR}_{Y|X} &amp; = \log{\frac{\text{Pr}(Y = 1 | X = 1, \textbf{C})}{1- \text{Pr}(Y = 1 | X = 1, \textbf{C})}} \\
                    &amp; \;\;\;\;\;\;\;\;\; - \log{\frac{\text{Pr}(Y = 1 | X = 0, \textbf{C})}{1- \text{Pr}(Y = 1 | X = 0, \textbf{C})}}
\end{aligned}`
$$

---
class: middle

## Conditioning on covariates


- So we have some statistical variable selection procedure. 
- Transfusion, and length of stay in hospital are on the causal pathway from `\(X\)` to `\(Y\)`, so they were excluded from adjustment.
- After adjustment of the other covariates, we have an estimated conditional logOR of &lt;br&gt; -0.24 (95% CI -0.41, -0.07)
- WE may interpret this logOR as evidence that patients treated with iron supplement before hip replacement surgery had lower odds of dying within 90 days of operation.

---
class: middle, center

## But, the conditional logOR still cannot answer our question: 


--
## What is the survival probability if, 

--
## **contrary to reality**, 

--
## every patient were treated with intravenous iron?


---
class: middle

## Causal languages

1. We ask the question: how `\(Y\)` would react if we could change `\(X\)` and assign it, **contrary to how it was actually assigned.** &lt;br&gt; -- potential outcomes &lt;br&gt;(Neyman, 1923; Rubin, D. 1974)

2. `\(Y(x)\)` is defined as the value that `\(Y\)` would take if `\(X\)` were (hypothetically) set to value of `\(x\)`

3. Causal effect is expressed as comparisons between `\(Y(x)\)`: &lt;br&gt; `\(E\{Y(1)\} - E\{Y(0)\}\)`


---
class: middle

## Potential Causal Estimands - binary outcome

- Marginal causal risk difference:

$$
\text{Pr}[Y(1) = 1] - \text{Pr}[Y(0) = 1]
$$

- Conditional causal log odds ratio:

$$
\log\frac{\text{Pr}(Y(1) = 1, \textbf{C})}{1- \text{Pr}(Y(1) = 1,  \textbf{C})} - \log\frac{\text{Pr}(Y(0) = 1, \textbf{C})}{1- \text{Pr}(Y(0) = 1,  \textbf{C})}
$$


---
class: middle

## Potential Causal Estimands - cont. outcome

- Marginal causal mean difference: 

`$$E\{Y(1) - Y(0)\}$$`
- Conditional causal mean difference:

`$$E\{Y(1) - Y(0)|\mathbf{C}\}$$`

They are called the **Average Causal/Treatment Effect (ACE or ATE)**. 

---
class: middle

## Assumption No 1: **No Interference**

- Potential value of `\(Y_i\)`, does not depend on `\(X_j\)`: 

    - `\(i, j\)`, are individual indices `\(\approx\)` independent

    - we assume that a (hypothetical) exposure for individual `\(j\)`, does not change the outcome of individual `\(i\)`


- An example of violation is in the study of vaccines, where vaccinating individual `\(j\)` may affect the disease status of individual `\(i\)`.


---
class: middle

## Assumption No 2: **Consistency**

$$
X_i = x \Rightarrow Y_i = Y_i(x)
$$

- For individual `\(i\)`, who actually (in the real world) received exposure `\(x\)`, their observed outcome is the same as in the hypothetical world that this individual received the same exposure `\(x\)`. 


---
class: middle 

## Assuption No 3: &lt;br&gt;**Conditional Exchangeability**

- This means we are so *arrogant* and 100% sure that there is **no other unmeasured confounding**.


`$$Y(x)\perp \!\!\! \perp X| \textbf{C}, \forall x$$`

- `\(\perp \!\!\! \perp\)` means conditional independence; 
    - `\(A\perp \!\!\! \perp B | C\)` means "A is conditionally independent of B given C"
    
- `\(\forall\)` means "for all", here means `\(x = 0, 1\)`


???

Conditional on `\(\mathbf{C}\)`, the actual exposure level `\(X\)` is independent of the level of the potential outcomes.


---
class: middle

## Identification (1): 

Suppose we are interested in a conditional causal mean difference. 

`$$E\{Y(1) - Y(0) | \mathbf{C} = \mathbf{c}\}$$`

Given `\(\mathbf{C}\)`, where `\(\mathbf{C}\)` also is a set of covariates given which we believe **conditional exchangaeability** to be plausible -- no other unmeasured confoundings.

---
class: middle

## Identification (2): 


`$$\begin{aligned}E\{Y(1) - Y(0) | \mathbf{C} = \mathbf{c}\} &amp; = E\{Y(1)| \mathbf{C} = \mathbf{c}\} \\
&amp;\;\;\;\;\;- E\{Y(0)| \mathbf{C} = \mathbf{c}\} \\
&amp; =  E\{Y(1)| \color{red}{X = 1}, \mathbf{C} = \mathbf{c}\} \\
&amp;\;\;\;\;\;- E\{Y(0)| \color{red}{X = 0}, \mathbf{C} = \mathbf{c}\}\\
&amp; (\text{conditional exchangeability}) \\ 
&amp; =  E\{\color{red}{Y}| X = 1, \mathbf{C} = \mathbf{c}\} \\
&amp;\;\;\;\;\;- E\{\color{red}{Y}| X = 0, \mathbf{C} = \mathbf{c}\} \\
&amp; (\text{by consistency}) \\ 
\end{aligned}$$`

???

These steps are extremely important, 
under the assumptions of conditional exchangeability, consistency, we have rewritten our causal estimand -- unobservable potential outcomes -- into the observed data.

---
class: middle

## Assumptions linked between &lt;br&gt; causal estimand and observed data

These steps are extremely important, 
under the assumptions of conditional exchangeability, consistency, we have rewritten our causal estimand 

-- unobservable potential outcomes -- 

into the observed data.


---
class: middle 

## Estimation using linear regression

Suppose

`$$E\{Y|x = 1, \mathbf{C} = \mathbf{c}\} - E\{Y|x = 0, \mathbf{C} = \mathbf{c}\}$$`

is the same for every `\(\mathbf{c}\)` (covariate), then if we fit a linear regression model:

`$$E(Y|X=x, \mathbf{C} = \mathbf{c}) = \alpha + \color{red}{\beta}x + \gamma^T\mathbf{c}$$`

--
then the coefficient `\(\color{red}{\beta}\)` of `\(X\)` can be interpreted as the conditional causal mean difference, as long as the model is correctly specified. 


---
class: middle 

### Example: &lt;br&gt; maternal smoking and birth weight (1)

- Data is from Cattaneo &lt;sup&gt;1&lt;/sup&gt; on singleton babies born in Pennsylvania between 1989 and 1991.

- Outcome: birth weight, in grams

- Exposure: whether or not mothers smoked during the pregnancy

- n = 4642






.footnote[
&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;font face="sans-serif"&gt;&lt;b&gt;&lt;br&gt;
[1]
 Cattaneo, M. D.

 Efficient semiparametric estimation of multi-valued treatment effects under ignorability 

 &lt;em&gt;Journal of Econometrics, &lt;/em&gt;



 &lt;em&gt;Elsevier, &lt;/em&gt;
&lt;b&gt;2010&lt;/b&gt;&lt;i&gt;, 155&lt;/i&gt;, 138-154 


&lt;p&gt;&lt;/p&gt;&lt;/font&gt;&lt;/body&gt;&lt;/html&gt;
]


---
class: middle
### Example: &lt;br&gt; maternal smoking and birth weight (2)

We think there are only 3 confounders:

- maternal age, 
- whether the baby is the first child,
- first clinical visit at which trimester. 

---
class: middle

### One possible linear regression model is:

.small[

```r
cattaneo2 &lt;- haven::read_dta("data/cattaneo2.dta")
Cat_mod &lt;- lm(bweight ~ as.factor(mbsmoke) + mage + 
                as.factor(fbaby) + as.factor(prenatal), 
              data = cattaneo2)
broom::tidy(Cat_mod, conf.int = TRUE) %&gt;% 
  knitr::kable(.)
```

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; p.value &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; conf.low &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; conf.high &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2735.8442 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 78.2260 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 34.9736 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2582.4841 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2889.2043 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; as.factor(mbsmoke)1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -252.2599 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 21.5677 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -11.6962 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -294.5428 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -209.9770 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; mage &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.2681 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.6146 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3.2627 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00111 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.1027 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.4336 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; as.factor(fbaby)1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -59.9184 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.7004 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -3.3851 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00072 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -94.6196 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -25.2172 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; as.factor(prenatal)1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 578.8464 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 68.5078 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.4494 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 444.5386 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 713.1542 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; as.factor(prenatal)2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 534.2280 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 70.6032 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7.5666 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 395.8122 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 672.6439 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; as.factor(prenatal)3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 458.5222 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80.9992 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.6608 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 299.7252 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 617.3191 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

---
class: middle
### Example: &lt;br&gt; maternal smoking and birth weight (3)

- Under assumptions of No Interference (NI),  Consistency (C), Conditional Exchangeability (CE) &lt;br&gt; and, **the model is correctly specified** 

- The estimate `\(\beta = -252.2599\)` can be given a **causal intepretation**: the expected difference in birth weight, conditional on three confounders, comparing a hypothetical situation in which: &lt;br&gt; &lt;br&gt; **all mothers smoke** versus a different hypothetical situation in which **no mothers smoke**


???

Under the assumptions, for a mother of a given age, first baby status, and timing of first clinical visit, we expect that the baby would be on average 252.3 grams lighter if the mother were hypothetically forced/randomised to smoking, versus if she were hypothetically prevented from smoking. 

---
class: middle

Under the assumptions, for a mother of a given age, first baby status, and timing of first clinical visit, we expect that the baby would be on average 252.3 grams lighter if the mother were **hypothetically forced/randomised to smoking**, versus if she were **hypothetically prevented from smoking**. 


---
class: middle

## Propensity score analysis (PSA)


- PSA make assumptions under the framework of causal inference: &lt;br&gt;No Interference (NI),  Consistency (C), Conditional Exchangeability (CE) 

--
- Plus, the (unknown) propensity score is appropriately modelled using the data.


--
- Instead of modelling `\(E(Y|X, \mathbf{C})\)`, we specify the form of `\(E(X| \mathbf{C})\)`


---
class: middle 

## Propensity score (1)

- The propensity score `\(p(\mathbf{C})\)`, is the conditional probability that `\(X = 1\)` given `\(\mathbf{C}\)`

$$
p(\mathbf{C}) = p(X = 1 | \mathbf{C})
$$

- A **scalar**, irrespective of the dimension of `\(\mathbf{C}\)`


- Rosenbaum and Rubin &lt;sup&gt;1&lt;/sup&gt; showed that if conditional exchangeability holds given `\(\mathbf{C}\)`, then it also holds given `\(p(\mathbf{C})\)`:

`$$Y(x)\perp \!\!\! \perp X| \textbf{C}, \forall x
\Rightarrow Y(x)\perp \!\!\! \perp X| \color{red}{p(\textbf{C})}, \forall x$$`


.tiny[
[1] &lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;font face="sans-serif"&gt; Rosenbaum, P. R. &amp;amp; Rubin, D. B. The central role of the propensity score in observational studies for causal effects
 &lt;em&gt;Biometrika, &lt;/em&gt;
 &lt;em&gt;Oxford University Press, &lt;/em&gt;
&lt;b&gt;1983&lt;/b&gt;&lt;i&gt;, 70&lt;/i&gt;, 41-55 


&lt;p&gt;&lt;/p&gt;&lt;/font&gt;&lt;/body&gt;&lt;/html&gt;
]


---
class: middle

## Propensity score (2)

- In practice,  `\(p(\mathbf{C})\)` must be estimated by fiting a logistic regression of `\(X\)` on `\(\mathbf{C}\)`; 

- The **predicted values** from the logistic model are the individual propensity scores. 

- The validity of methods based on propensity scores relies on **correctly modelling** `\(E(X| \mathbf{C})\)`


---
class: middle

## Propensity score (3)

- If an exposed and unexposed person have the same value of `\(p(\mathbf{C})\)`, say 0.25, it means they were equally likely to have received the exposure. 

- This is **similar as** in a randomised trial, an exposed and unexposed subject with the same `\(p(\mathbf{C})\)` are exchangeable. Unless: 

  - Important confounders were not included in `\(p(\mathbf{C})\)`; 
  - `\(p(\mathbf{C})\)` was incorrectly modelled. 


---
class: middle

## Example: RFA dataset (1)

--
- 3351 patients with metastatic lung cancer were given either standard surgery (n = 1848) or radiofrequency ablation (RFA) (n = 1703) to remove metastatic lung cancer nodules. 

--
- 3-year progression-free survival is higher (79.2% versus 67.9%) for those who received RFA. 

--
- But confounding is a concern: larger modules cannot be removed by RFA, so at least some of the apparent protective effect of RFA is likely due to RFA given to patients **with already better prognosis**.


---
class: middle

## Example: RFA dataset (2)

- Potential confounders: age, gender, hospital (1,2,3,4), smoking (non, ex, current), nodules numbers, no. other metastatic sites, duration of disease, diameter of largest module, location of primary cancer (bladder, breast, bowel, gullet, kidney, skin, stomach....), modules can be reached easily (easy, moderate, difficult) 

---
class: middle

## Example: RFA dataset (3)

- Propensity score model: 


.small[
$$
`\begin{aligned}
\text{logit}\{ \text{Pr(RFA}|\mathbf{C} )\}  = &amp; \beta_0 + \beta_1\text{age} + \beta_2 \text{gender}+ \beta_3I(\text{hospital = 2}) \\
&amp; +\beta_4I(\text{hospital =3}) + \beta_5I(\text{hospital = 4}) \\
&amp; + \beta_6I(\text{smoke = 2} ) + \beta_7I(\text{smoke = 3}) + \beta_8\text{nodules}\\
&amp;  + \beta_9\text{mets} + \beta_{10}\text{duration}+ \cdots  + \beta_{20}I(\text{primary = 9})\\
&amp;  + \beta_{21}I(\text{position = 2}) + \beta_{22}I(\text{position = 3})
\end{aligned}`
$$
]

- `\(p(\mathbf{C})\)` is estimated from this `\(\uparrow\)` model for every patient.


---
class: middle

### The PS model estimates: 


.med[
&lt;div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:400px; overflow-x: scroll; width:780px; "&gt;&lt;table class="table table-striped" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;"&gt; term &lt;/th&gt;
   &lt;th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;"&gt; estimate &lt;/th&gt;
   &lt;th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;"&gt; std.error &lt;/th&gt;
   &lt;th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;"&gt; statistic &lt;/th&gt;
   &lt;th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;"&gt; p.value &lt;/th&gt;
   &lt;th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;"&gt; conf.low &lt;/th&gt;
   &lt;th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;"&gt; conf.high &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; (Intercept) &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2.95799 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.61771 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.78866 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.74981 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 4.17252 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; age &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00044 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00993 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.04434 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.96463 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.01993 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.01902 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; gender1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.10435 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.15103 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.69093 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.48961 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.19160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.40063 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; smoke1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.11862 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.15107 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.78518 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.43235 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.17753 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.41508 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; smoke2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.09342 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.10381 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.89997 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.36814 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.11000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.29704 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; hospital2 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; -0.23564 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.14244 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; -1.65436 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.09805 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; -0.51512 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.04341 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; hospital3 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.30046 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.13851 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 2.16923 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.03007 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.02923 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.57236 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; hospital4 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; -0.11175 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.19237 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; -0.58092 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.56129 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; -0.48898 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.26526 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; nodules &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.02988 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.01821 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -1.64105 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.10079 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.06594 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00550 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; mets &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.06514 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.04672 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.39423 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.16325 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.02641 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.15680 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; duration &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00261 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00489 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.53354 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.59366 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00698 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.01218 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; maxdia &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; -2.26045 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.08774 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; -25.76373 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; -2.43490 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; -2.09089 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; primary2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.09928 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.29102 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.34114 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.73300 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.46959 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.67330 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; primary3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.14593 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.29460 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.49536 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.62035 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.42937 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.72768 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; primary4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00213 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.38021 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.00559 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.99554 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.74649 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.74543 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; primary5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.22135 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.37933 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.58353 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.55953 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.96499 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.52355 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; primary6 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.04505 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.34609 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.13016 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.89644 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.63128 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.72712 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; primary7 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 1.57439 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.64793 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 2.42989 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.01510 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.36040 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 2.92946 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; primary8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.21261 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.38273 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.55552 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.57854 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.53651 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.96525 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; primary9 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.33223 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.40566 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.81899 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.41279 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; -0.46107 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.13046 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; position2 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.94096 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.10080 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 9.33491 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.74445 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 1.13969 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; position3 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 1.22135 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.11678 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 10.45840 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.00000 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 0.99365 &lt;/td&gt;
   &lt;td style="text-align:right;font-weight: bold;color: white !important;background-color: #D7261E !important;"&gt; 1.45155 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/div&gt;
]


???

As expected, hospital, diameter of largest nodule, and position of the nodule are strong predictors of the treatment. 


---
class: inverse
background-image: url("./fig/scoreoverlap.png")
background-position: 50% 50%
background-size: contain




---
class: middle

## How do we use the propensity scores?

- If controlling `\(\mathbf{C}\)` is deemed sufficient, then controlling for `\(p(\mathbf{C})\)` is also sufficient. 

- `\(p(\mathbf{C})\)` is a scalar, so controlling for it is much easier than controlling for `\(\mathbf{C}\)`.

- Ways of using the propensity scores include: 
  - stratification;
  - adjustment;
  - matching;
  - re-weighting (inverse-weighting).


---
class: middle

### Stratification (1)

If we subclassifying the patients according to quartiles of `\(\hat{p}(\mathbf{C})\)`: 

.small[
| PS subclasses | Treatment | N of patients | Prob of death or &lt;br&gt;disease progression (%) | Risk difference (%) |
|:-------------:|:---------:|:-------------:|:--------------------------------------------:|:-------------------:|
|       1       |  Standard |      788      |                     37.9                     |         13.1        |
|               |    RFA    |      100      |                     51.0                     |                     |
|       2       |  Standard |      547      |                     30.7                     |         6.0         |
|               |    RFA    |      341      |                     36.7                     |                     |
|       3       |  Standard |      344      |                     25.6                     |         -4.8        |
|               |    RFA    |      544      |                     20.8                     |                     |
|       4       |  Standard |      169      |                     22.5                     |        -13.4        |
|               |    RFA    |      718      |                      9.1                     |                     |
]

---
class: middle

### Stratification (2)

- Our suspicion of confounding is seen clearly, the probability of death or disease progression is higher in the lower categories of the propensity score. 

- There is effect modification by the propensity score. RFA is better than standard surgery only in the upper two propensity score subclasses. 

- The doctors are making choices for their patients. RFA are more likely to be given to patients who can benefit from the treatment.


---
class: middle

### Estimating  `\(E\{Y(1) - Y(0)\}\)` &lt;br&gt; (average causal/treatment effect, ACE)


The ACE can be estimated as the simple (unweighted) average of the 4 stratum-specific effects:


$$
\widehat{\text{ACE}} = \frac{13.1 + 6.0 - 4.8 - 13.4}{4} = 0.2 \%
$$

---
class: middle

### Estimating `\(E\{Y(1) - Y(0) | X = 1\}\)` &lt;br&gt; (average treatment effect on the **treated**, ATT)


.between[
| PS subclasses | Treatment | N of patients | Prob of death or &lt;br&gt;disease progression (%) | Risk difference (%) |
|:-------------:|:---------:|:-------------:|:--------------------------------------------:|:-------------------:|
|       1       |  Standard |      788      |                     37.9                     |         13.1        |
|               |    RFA    |      **100**      |                     51.0                     |                     |
|       2       |  Standard |      547      |                     30.7                     |         6.0         |
|               |    RFA    |      **341**      |                     36.7                     |                     |
|       3       |  Standard |      344      |                     25.6                     |         -4.8        |
|               |    RFA    |      **544**      |                     20.8                     |                     |
|       4       |  Standard |      169      |                     22.5                     |        -13.4        |
|               |    RFA    |      **718**      |                      9.1                     |                     |
]

.small[
$$
\widehat{\text{ATT}} = \frac{-13.1 \times 100 + 6.0 \times 341 - 4.8 \times 544 - 13.4\times718}{100 + 341 + 544 + 718} = -5.2\%
$$
]

???

The results shows that RFA is beneficial to those who tend to get the treatment (consistent with the effect modification)


---
class: middle, center

# Hold on,

--
# Before we move on to other methods of using the PS. 

--
# We should bear in mind that we assumed the model for PS generation was correctly chosen. 

---
class: middle, center

## Other than that, 

--
## we also assumed that **all confounders** were included 

--
## plus, some people may believe including non-confounders but predictive of the X is beneficial for precision. 


---
class: middle


## Finally,

--
## How do we know if there is any other un-observed confounders?




---
class: middle
## Check for un-observed confounders (1)

- Researchers should routinely test for residual confounding or **endogeneity** (even after PS matching). &lt;br&gt;
This can be done using residuals of the PS model.

.med[
$$
`\begin{aligned}
\text{logit}\{ \text{Pr(RFA}|\mathbf{C}) \} &amp; =  \beta_0 + \beta_1\text{age} + \dots + \color{red}{\epsilon}
\end{aligned}`
$$
]


---
class: middle
## Check for un-observed confounders (2)

- If the outcome equation is as follows (linear model here for illustration), where `\(\nu\)` is the residuals.

.med[
$$
`\begin{aligned}
Y &amp; =  \alpha_0 + \alpha_1\text{age} + \dots + \color{red}{\nu}
\end{aligned}`
$$
]

- The following assumption should hold 

`$$\small{cor(\text{logit}\{ \text{Pr(RFA}|\mathbf{C}) \},\nu)=cor(\epsilon,\nu)=0}$$`


---

class: middle

## Hausman test (Endogeneity test)

- Null hypothesis is as follows: 

`$$\small{cor(\text{logit}\{ \text{Pr(RFA}|\mathbf{C} )\},\nu)=cor(\epsilon,\nu)=0}$$`

- Equivalent to test `\(\color{blue}{\delta}=0\)` in the following equation (using usual tests):

$$
`\begin{aligned}
Y =  \alpha_0 + \alpha_1\text{age} + ...+\color{blue}{\delta} \epsilon +\color{red}{\nu}
\end{aligned}`
$$


---
class: middle

## Alternative #1: (1)&lt;br&gt; Differences-in-Differences methods

Other than fitting models to calculate propensity scores, we may do the following:
&lt;!-- to propensity score based methods --&gt;

- Non-parametric alternative

- Require 2 periods in a same patient: 

1.  A baseline period in the same period when neither the intervention nor the comparator are observed
2.  A period the intervention or the comparator is attributed


---
class: middle

## Alternative #1: (2)

- The outcome equations for the respective periods are:


$$
`\begin{aligned}
Y_{i1} &amp; =  B_0 + B_1 X_{i1}+\mathbf{B_2 \lambda_{i}} +\epsilon_{i1} \\
Y_{i2} &amp; = B_0 + B_1 X_{i2}+\mathbf{B_2 \lambda_{i}} + \color{red}{B_3 T_i}+\epsilon_{i2}
\end{aligned}`
$$


- The effect for each patient is estimated as follows:

`$$Y_{i2}-Y_{i1} = B_1(X_{i2}-X_{i1})+B_3T_i+(\epsilon_{i2}-\epsilon_{i1})$$`


---
class: middle

### Pros

- remove unobserved fixed effects

- instrumental variables are not required

- possibility to combine DID with PS matching (**Heckman et al.**)

### Cons

- DID estimator interpretation is not clear in the presence of within-treatment heterogeneity (differential effect in subgroups of non-observed characteristics)

---
class: middle

## Alternative #2: Endogenous switching regression (Econometric approach)


- Use in practice when heterogeneity is observed

- Compared to propensity score model, the selection process is modeled and counterfactuals are considered to estimate the effects

- ATE (average treatment effect), ATT, **ATU** (Average treatment effect on untreated population), heterogeneity for each outcome



---
class: middle

## Definitions of the different effects

- Average treatment among the untreated

.med[
$$
ATU: E(Y(1) - Y(0)|\mathbf{X,W=0})
$$
]

- Effect of heterogeneity in the treated

.med[
$$
BH1: E(Y(0)|\mathbf{X,W=1})-E(Y(0)|\mathbf{X,W=0})
$$
]

- Effect of heterogeneity in the untreated

.med[
$$
BH2: E(Y(1)|\mathbf{X,W=1})-E(Y(1)|\mathbf{X,W=0})
$$
]

---
class: middle

### General model framework (1)

- The selection process is based on the estimation of utility:

`\(A_i^{*}=Z_i\alpha+\eta_i\)` **(1)**, with `\(Z_i\)` as observed variables (at least one instrument is required)

.small[
$$
A_i=
`\begin{cases}
1 &amp; \text{ if } A_i^{*}&gt;0\\
0 &amp; \text{otherwise}
\end{cases}`
$$
]

---
class: middle

### General model framework (2)

- In a second step, a switching regression model **(2)** is estimated for each subpopulation `\(A_i\)`:


$$
`\begin{aligned}
Y_{1i} &amp; =  X_{1i}\beta_1+\epsilon_{1i} \\
Y_{0i} &amp; =  X_{0i}\beta_0+\epsilon_{0i}
\end{aligned}`
$$

- Correlations between residuals in **(1)** and **(2)** are nonzero (presence of heterogeneity).

$$
Cor(\eta,\epsilon) \neq 0
$$


---
class: middle

### Pros

- Assessment of ATU and BH effects: policy/intervention on untreated and heterogeneity might be important in decision-making

- Control for both the structural relationships of the covariates with outcomes, and the sample composition of both groups

### Cons

- Difficult to find a good instrument in practice (variable included in the selection process not related to the outcome directly)



---
class: middle, center 

# The end for the first day

## slide address: https://wangcc.me/PSA-CSS


---
class: middle, center, inverse

# Propensity Score Analysis (PSA) -- Day 2
### Chaochen Wang (CWAN) &lt;br&gt; Thomas Laurent (TLAU)

### 2020-2-28 14:30~15:30 @CSS


---
class: middle

# Recap

We discussed about:

- the causal inference framework under which that PSA was designed for; 

- the assumptions required when consider using the PSA;

    - No interference, Consistency, Conditional Exchangeability

- ATE calculated through a stratification procedure. 

---
class: middle

# Today 

We will try to cover: 

- Adjusting for PS in the model; 

- Matching participants by PS; 

- Inversely weighting the participants by their PS;

&lt;!-- - Cautions and best guidelines to follow when reporting studies used PS.  --&gt;

---
class: middle

## Regression Adjustment

- Another approach of using PS is to **adjust** for the propensity score in a regression model.

`$$E\{Y|X, p(\mathbf{C})\} = \alpha + \color{red}{\beta} X + \gamma p(\mathbf{C})$$`


- `\(\color{red}{\beta}\)` potentially has a **causal (conditional) interpretation**

    - because if conditional exchangeability holds given `\(\mathbf{C}\)` then it also holds given `\(p(\mathbf{C})\)`
    
- `\(\gamma\)` is now one-dimensional so, **finite sample bias** would no longer be a problem.


  
---
class: inverse
background-image: url("./fig/adjustment.png")
background-position: 50% 50%
background-size: contain


???

finite sample bias is no longer a concern for propensity score adjustment as the number of covariates increases. 


---
class: middle

## Matching on the estimated PS

Matching on the PS means we take one patient who recieved RFA and find one (or more than one) match for him/her **with replacement** from those who recieved standard surgery but with a similar value of PS.

- To estimate **average treatment effect in the treated (ATT)**;


- If we start matching with those who recieved standard surgery, this will be an estimate of the **average treatment effect in the untreated (ATU)**.


---
class: middle

## Matching methods

- nearest neightbour matching

- within calipers method (Mahalanobis metric matching)

- etc.

- Problem of **poor overlap (or lack of positivity)** is immediately flagged up when some individuals fail to be mached. 

---
class: middle

## Balance diagnostics after matching

- [Standardized mean difference (SMD)](https://cran.r-project.org/web/packages/tableone/vignettes/smd.html) is mostly used: 

`$$SMD = \frac{\bar{X_1} - \bar{X_2}}{\sqrt{(\hat{\sigma_1}^2 + \hat{\sigma_2}^2)/2}} &lt;0.1$$`

- For dichotomous variables and categorical variable, see reference &lt;sup&gt;1&lt;/sup&gt;: 

.between[
Yang DS, Dalton JE. A Unified Approach to Measuring the Effect Size Between Two Groups Using SAS. SAS Global Forum 2012. paper 335
]

---
class: middle

## Inverse probability weighting (1)

- The propensity scores we have calculated are the **conditional probabilities** - based on all of the confounders we have found and included - that the individuals will be exposed. 

- So there are some individuals, who, condition on their confounders, are more or less likely to be exposed. 

---
class: middle 

## Inverse probability weighting (2)

- If we found that someone in the study, condition on her/his all available confounders, was **unlikely** to be exposed to the intervention (treatment), `\(p(\mathbf{C})\)` is small.

--
- We may consider **upweight** him/her, so that (s)he represents him/her and also many other who may like him/her who were unlikely to be unexposed.

--
- Then we will have a re-weighted dataset in which `\(\mathbf{X}\)` (exposure), and `\(p(\mathbf{C})\)` are independent, but everything else is unchanged. 


---
class: middle 

## Simple example (1)

- Suppose the counterfactual data are:

|      Group     | A | A | A | B | B | B | D | D | D |
|:--------------:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
| Response `\(Y(1)\)`: | 1 | 1 | 1 | 2 | 2 | 2 | 3 | 3 | 3 |
| Response `\(Y(0)\)`: | 0 | 0 | 0 | 1 | 1 | 1 | 2 | 2 | 2 |

- Even without fitting any model, we can tell that the average treatment effect (ATE) is 1. 

---
class: middle

## Simple example (2)

- However, we can only observe (in the real world under consistency):

|     Group    |  A |  A |  A |  B |  B |  B |  D |  D |  D |
|:------------:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|  Exposed `\(Y\)`:  |  1 |  1 | NA | NA |  2 | NA |  3 | NA | NA |
| Unexposed `\(Y\)`: | NA | NA |  0 |  1 | NA |  1 | NA |  2 |  2 |


- The estimated average treatment effect (ATE) is &lt;br&gt; 7/4 - 6/5 = 0.55, **(hugely biased)**


---
class: middle

## Weighting to reduce bias (IPTW)

- The probability of recieving treatment: 

  - 2/3 in group A; 
  - 1/3 in group B;
  - 1/3 in group D.

- Calculate weighted average &lt;br&gt; (by **1/{Probability of observed treament}**)


.small[
`$$\frac{(1 + 1) \times \frac{3}{2} + (2) \times \frac{3}{1} + (3) \times \frac{3}{1}}{\frac{3}{2} + \frac{3}{2} + \frac{3}{1} + \frac{3}{1}} \\ \;\;\;\;- \frac{(0) \times \frac{3}{1} + (1 + 1) \times \frac{3}{2} + (2 + 2) \times \frac{3}{2}}{\frac{3}{1} + \frac{3}{2}+ \frac{3}{2}+ \frac{3}{2}}= 2-1 = 1$$`
]

---
class: middle

## **I**nverse **P**robability of **T**reatment **W**eighting (IPTW)

- IPTW successfully removed the bias by creating a "fake" population where we 'observe' each subject at each exposure level.

- It can be proved that IPTW can provide consistent estimators. 

---
class: middle

## IPTW conditioning on binary confounder C (1)

.pull-left[
- Observed data: 

&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;border-width:1px;border-style:solid;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-p0ii{background-color:#f9f9f9;font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-9d8n{font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
&lt;/style&gt;
&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-9d8n"&gt;&lt;/th&gt;
    &lt;th class="tg-9d8n" colspan="2"&gt;Y = 1&lt;/th&gt;
    &lt;th class="tg-9d8n" colspan="2"&gt;Y = 0&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;C = 1&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;C = 0&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;C = 1&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;C = 0&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;X = 1&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;180&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;200&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;600&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;200&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;X = 0&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;20&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;200&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;200&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;600&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
]



.pull-right[
- Table of `\(\color{red}{P(X|C)}\)` 

&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;border-width:1px;border-style:solid;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-p0ii{background-color:#f9f9f9;font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-9d8n{font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
&lt;/style&gt;
&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-9d8n"&gt;&lt;/th&gt;
    &lt;th class="tg-9d8n" colspan="2"&gt;Y = 1&lt;/th&gt;
    &lt;th class="tg-9d8n" colspan="2"&gt;Y = 0&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;C = 1&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;C = 0&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;C = 1&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;C = 0&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;X = 1&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;0.780&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;0.333&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;0.780&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;0.333&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;X = 0&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;0.220&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;0.667&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;0.220&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;0.667&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

]

Where, `\(0.780 = (180 + 600)/(180 + 600 + 20 + 200)\)`


---
class: middle 
## IPTW conditioning on binary confounder C (2)

- Dividing each cell count by `\(\color{red}{P(X|C)}\)` 

&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;border-width:1px;border-style:solid;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-p0ii{background-color:#f9f9f9;font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-9d8n{font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
&lt;/style&gt;
&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-9d8n"&gt;&lt;/th&gt;
    &lt;th class="tg-9d8n" colspan="2"&gt;Y = 1&lt;/th&gt;
    &lt;th class="tg-9d8n" colspan="2"&gt;Y = 0&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;C = 1&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;C = 0&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;C = 1&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;C = 0&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;X = 1&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;180/0.78 &lt;br&gt;= 231&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;200/0.333 &lt;br&gt;= 600&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;600/0.78 &lt;br&gt;= 769&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;200/0.333 &lt;br&gt;= 600&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;X = 0&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;20/0.22 &lt;br&gt;= 91&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;200/0.667 &lt;br&gt;= 300&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;200/0.22 &lt;br&gt;= 909&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;600/0.667 &lt;br&gt;= 900&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

---
class: middle
## IPTW conditioning on binary confounder C (3)

- The impact of inverse weighting by `\(\color{red}{P(X|C)}\)` is to create a pseudo(fake)-population where we 'observe' each subject at each exposure level: 

&lt;style type="text/css"&gt;
.tg  {border-collapse:collapse;border-spacing:0;border-width:1px;border-style:solid;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-p0ii{background-color:#f9f9f9;font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-390v{background-color:#f8ff00;font-size:22px;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-9d8n{font-size:22px;border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-ovs6{font-size:22px;background-color:#f8ff00;border-color:inherit;text-align:left;vertical-align:top}
&lt;/style&gt;
&lt;table class="tg"&gt;
  &lt;tr&gt;
    &lt;th class="tg-9d8n"&gt;&lt;/th&gt;
    &lt;th class="tg-9d8n" colspan="2"&gt;Y = 1&lt;/th&gt;
    &lt;th class="tg-9d8n" colspan="2"&gt;Y = 0&lt;/th&gt;
    &lt;th class="tg-9d8n" colspan="2"&gt; Total &lt;/th&gt;
    &lt;!-- &lt;th class="tg-0pky"&gt;&lt;/th&gt; --&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;C = 1&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;C = 0&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;C = 1&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;C = 0&lt;/td&gt;
    &lt;td class="tg-390v"&gt;C = 1&lt;/td&gt;
    &lt;td class="tg-ovs6"&gt;C = 0&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;X = 1&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;231&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;600&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;769&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;600&lt;/td&gt;
    &lt;td class="tg-390v"&gt;1000&lt;/td&gt;
    &lt;td class="tg-ovs6"&gt;1200&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td class="tg-9d8n"&gt;X = 0&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;91&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;300&lt;/td&gt;
    &lt;td class="tg-p0ii"&gt;909&lt;/td&gt;
    &lt;td class="tg-9d8n"&gt;900&lt;/td&gt;
    &lt;td class="tg-390v"&gt;1000&lt;/td&gt;
    &lt;td class="tg-ovs6"&gt;1200&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

---
class: middle

# PS matching and IPTW example - data

- Right heart catheterization data

- We can download the data from : [http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/rhc.csv)

- Data are from ICU patients in 5 hospitals

- **Treatment**: right heart catheterization (rhc) or not

- **Outcome**: death

- **Confounders**: demographics, insurance, disease diagnoses, etc.

???
catheterization: カテーテル
&lt;!-- (https://www.coursera.org/lecture/crash-course-in-causality/data-example-in-r-Ie48W) --&gt;


---
class: middle

## First steps &lt;br&gt;- load packages, read in data


```r
# load packages
library(tableone)
library(Matching)

# Read in data
load(url("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/rhc.sav"))
```


---
class: middle

## Second steps - create data set

.small[

```r
# create a data set with just these variables, for simplicity
library(tidyverse) # load this package if you have not

dat &lt;- rhc %&gt;% 
  dplyr::select(cat1, sex, death, age, swang1, meanbp1) %&gt;% 
  mutate(ARF = as.numeric(cat1 == "ARF"), 
         CHF = as.numeric(cat1 == "CHF"), 
         Cirr = as.numeric(cat1 == "Cirrhosis"), 
         colcan = as.numeric(cat1 == "Colon Cancer"), 
         Coma = as.numeric(cat1 == "Coma"), 
         COPD = as.numeric(cat1 == "COPD"), 
         lungcan = as.numeric(cat1 == "Lung Cancer"), 
         MOSF = as.numeric(cat1 == "MOSF w/Malignancy"), 
         sepsis = as.numeric(cat1 == "MOSF w/Sepsis"), 
         female = as.numeric(sex == "Female"), 
         died = as.numeric(death == "Yes"), 
         treatment = as.numeric(swang1 == "RHC"), 
         age = as.numeric(age), 
         meanbp1 = as.numeric(meanbp1))
```
]


---
class: middle 

## Continued

.small[

```r
# Covariates will be used: 
xvars &lt;- c("ARF", "CHF", "Cirr", "colcan", "Coma", "lungcan", "MOSF", 
           "sepsis", "age", "female", "meanbp1")
```
]

### Create a Table 1

.small[

```r
# Look at a table 1

table1 &lt;- CreateTableOne(vars = xvars, strata = "treatment", 
                         data = dat, test = FALSE)

# include standardized mean difference (SMD)
print(table1, smd = TRUE)
```
]

---
class: middle

### Table 1, unmatched

.small[
```r
                     Stratified by treatment
                      0             1             SMD   
  n                    3551          2184               
  ARF (mean (SD))      0.45 (0.50)   0.42 (0.49)   0.059
  CHF (mean (SD))      0.07 (0.25)   0.10 (0.29)   0.095
* Cirr (mean (SD))     0.05 (0.22)   0.02 (0.15)   0.145
  colcan (mean (SD))   0.00 (0.04)   0.00 (0.02)   0.038
* Coma (mean (SD))     0.10 (0.29)   0.04 (0.20)   0.207
  lungcan (mean (SD))  0.01 (0.10)   0.00 (0.05)   0.095
  MOSF (mean (SD))     0.07 (0.25)   0.07 (0.26)   0.018
* sepsis (mean (SD))   0.15 (0.36)   0.32 (0.47)   0.415
  age (mean (SD))     61.76 (17.29) 60.75 (15.63)  0.061
  female (mean (SD))   0.46 (0.50)   0.41 (0.49)   0.093
* meanbp1 (mean (SD)) 84.87 (38.87) 68.20 (34.24)  0.455
```
]


???

- A few variables showed some imbalanced. (SMD &gt; 0.1)


---
class: middle

### Match

- Greedy matching based on [Mahalanobis distance](https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%8F%E3%83%A9%E3%83%8E%E3%83%93%E3%82%B9%E8%B7%9D%E9%9B%A2) 

.small[

```r
# do greedy matching
greedymatch &lt;- Match(Tr = dat$treatment, 
                     M = 1, # 1:1
                     X = dat[xvars])

matched &lt;- dat[unlist(greedymatch[c("index.treated", "index.control")]), ]
matchedtab1 &lt;- CreateTableOne(vars = xvars, strata = "treatment", 
                              data = matched, test = FALSE)

print(matchedtab1, smd = TRUE)
```
]

---
class: middle

### Matched data Table 1

.small[
```r
                     Stratified by treatment
                      0             1             SMD   
  n                    2186          2186               
  ARF (mean (SD))      0.42 (0.49)   0.42 (0.49)  &lt;0.001
  CHF (mean (SD))      0.10 (0.29)   0.10 (0.29)  &lt;0.001
  Cirr (mean (SD))     0.02 (0.15)   0.02 (0.15)  &lt;0.001
  colcan (mean (SD))   0.00 (0.02)   0.00 (0.02)  &lt;0.001
  Coma (mean (SD))     0.04 (0.20)   0.04 (0.20)  &lt;0.001
  lungcan (mean (SD))  0.00 (0.05)   0.00 (0.05)  &lt;0.001
  MOSF (mean (SD))     0.07 (0.26)   0.07 (0.26)  &lt;0.001
  sepsis (mean (SD))   0.32 (0.47)   0.32 (0.47)  &lt;0.001
  age (mean (SD))     60.84 (15.54) 60.77 (15.64)  0.005
  female (mean (SD))   0.41 (0.49)   0.41 (0.49)  &lt;0.001
  meanbp1 (mean (SD)) 68.26 (33.23) 68.19 (34.23)  0.002
```
]

---
class: middle

### Outcome analysis -  causal risk difference&lt;br&gt;by simply performing a paired t-test

.small[

]

.small[

```r
# outcome analysis
y_trt &lt;- matched$died[matched$treatment == 1]
y_con &lt;- matched$died[matched$treatment == 0]

# pairwise difference
diffy &lt;- y_trt - y_con

# paired t-test
t.test(diffy)
```

```
## 
## 	One Sample t-test
## 
## data:  diffy
## t = 3.33, df = 2185, p-value = 0.00088
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.018632 0.071944
## sample estimates:
## mean of x 
##  0.045288
```
]

---
class: middle

### Causal risk difference - summary

- Point estimate: 0.045

    - Difference in probability of death **if everyone received RHC** compared with **if no one recieved RHC** is 0.045 (i.e., higher risk of death in RHC group)

- 95% CI: (0.019, 0.072)

- P-value: &lt; 0.001

- don't take it too seriously, this is just for illustration, we have not controlled for all of the confounders. 

---
class: middle

### McNemar test

.small[

```r
# McNemar test

table(y_trt, y_con)
```

```
##      y_con
## y_trt   0   1
##     0 305 394
##     1 493 994
```

```r
mcnemar.test(matrix(c(994, 493, 394, 305), 2, 2))
```

```
## 
## 	McNemar's Chi-squared test with continuity correction
## 
## data:  matrix(c(994, 493, 394, 305), 2, 2)
## McNemar's chi-squared = 10.8, df = 1, p-value = 0.001
```
]


---
class: middle

### McNemar test - summary

- 493 + 394 pairs are discordant

- 493 means when a treated person died and a control person did not

- P value &lt; 0.001 

- Same conclusion from the paried t-test &lt;br&gt;-- treated persons were at higher risk of death


---
class: middle

# IPTW - example using the same data

- load packages, read in data

.small[

```r
# load packages
library(tableone)
*library(sandwich)
    # for robust variance estimation
*library(ipw)
*library(survey)

# Read in data
load(url("http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/rhc.sav"))
```
]

---
class: middle

## Calculate the propensity score

.small[

]

.small[

```r
# propensity score model
psmodel &lt;- glm(treatment ~ age + female + meanbp1 + ARF + 
                 CHF + Cirr + colcan + Coma + lungcan + 
                 MOSF + sepsis + aps, 
               family = binomial(link = "logit"), 
               data = dat)

# value of propensity score for each subject
ps &lt;- predict(psmodel, type = "response")
```
]

---
class: middle

## Results from the PS model

.small[
```r
Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.946015   0.232129   -8.38  &lt; 2e-16 ***
*age         -0.003047   0.001746   -1.74  0.08101 .  
*female      -0.139077   0.059014   -2.36  0.01844 *  
*meanbp1     -0.007517   0.000871   -8.63  &lt; 2e-16 ***
*ARF          1.225293   0.149551    8.19  2.5e-16 ***
*CHF          1.890564   0.173569   10.89  &lt; 2e-16 ***
Cirr         0.433406   0.220337    1.97  0.04918 *  
colcan       0.048157   1.124289    0.04  0.96583    
*Coma         0.684254   0.187833    3.64  0.00027 ***
lungcan      0.198460   0.505500    0.39  0.69461    
*MOSF         1.017780   0.180716    5.63  1.8e-08 ***
*sepsis       1.840246   0.156159   11.78  &lt; 2e-16 ***
*aps          0.018236   0.001729   10.55  &lt; 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7621.4  on 5734  degrees of freedom
Residual deviance: 6869.2  on 5722  degrees of freedom
AIC: 6895

Number of Fisher Scoring iterations: 4
```
]

???
You probably will be interested on which variables are associated with receiving the treatment in the data.

- age negative coefficient


---
class: inverse
background-image: url("./fig/rhcpreweighting.png")
background-position: 50% 50%
background-size: contain




???
good overlapping between two treatment groups


---
class: middle

## Create weights and check balance

.small[

```r
# Create weights
dat &lt;- dat %&gt;% 
  mutate(weight = if_else(treatment == 1, 1/(ps), 1/(1-ps)))

# apply weights to data
*weighteddata &lt;- svydesign(ids = ~1, data = dat, weights = ~ weight)

# weighted table 1

weightedtable &lt;- svyCreateTableOne(vars = xvars, strata = "treatment", 
                                   data = weighteddata, test = FALSE)
```
]

.small[

```r
## Show table with SMD

print(weightedtable, smd = TRUE)
```
]


---
class: middle
## Check balance

- numbers in the brackets `()` are standard deviations but we can ignore them.

.small[
```r
                    Stratified by treatment
                      0               1               SMD   
  n                   5760.80         5660.88               
  ARF (mean (SD))        0.43 (0.50)     0.45 (0.50)   0.021
  CHF (mean (SD))        0.08 (0.27)     0.08 (0.27)   0.001
  Cirr (mean (SD))       0.04 (0.19)     0.04 (0.18)   0.017
  colcan (mean (SD))     0.00 (0.04)     0.00 (0.06)   0.039
  Coma (mean (SD))       0.07 (0.26)     0.07 (0.25)   0.025
  lungcan (mean (SD))    0.01 (0.08)     0.01 (0.08)   0.010
  MOSF (mean (SD))       0.07 (0.25)     0.07 (0.26)   0.008
  sepsis (mean (SD))     0.22 (0.41)     0.22 (0.41)   0.004
  age (mean (SD))       61.37 (17.59)   61.52 (15.22)  0.010
  female (mean (SD))     0.44 (0.50)     0.44 (0.50)  &lt;0.001
  meanbp1 (mean (SD))   78.28 (38.20)   78.14 (38.34)  0.004
```
]

???

because these standard deviations are based on the weighted data, 
which shows as a weighted sample size. They are not real sample size so sd does not really mean anything.

The means of the pseudo population is actually what we want. 

You can see the weighted mean are very well balanced.


---
class: middle 

## You can also get weighted mean in a hard way

`$$\frac{\sum_{i = 1}^n I(A_i = 1) \frac{X_i}{\pi_i}}{\sum_{i = 1}^n \frac{I(A_i = 1)}{\pi_i}}$$`

.small[

```r
dat %&gt;% 
  dplyr::filter(treatment == 1) %&gt;% 
  dplyr::select(treatment, weight, age) %&gt;% 
  mutate(WtedAge = mean(weight*age)/mean(weight)) 
```

```
##     treatment  weight    age WtedAge
## 1           1  2.0458 78.179  61.524
## 2           1  2.1513 46.092  61.524
## 3           1  1.5995 67.910  61.524
## 4           1  2.9753 48.424  61.524
## 5           1  2.4814 68.348  61.524
## 6           1  1.8202 74.710  61.524
## 7           1  2.3195 88.422  61.524
## 8           1  2.2953 69.002  61.524
## 9           1  1.5343 50.590  61.524
## 10          1  3.4034 62.689  61.524
## 11          1  1.9898 42.053  61.524
## 12          1  4.4031 39.830  61.524
## 13          1  1.5942 71.496  61.524
## 14          1  1.8736 61.440  61.524
## 15          1  2.3290 51.400  61.524
## 16          1  1.7569 21.092  61.524
## 17          1  2.6991 56.769  61.524
## 18          1  1.6371 83.370  61.524
## 19          1  2.8080 61.867  61.524
## 20          1  2.4672 85.320  61.524
## 21          1  3.3861 47.975  61.524
## 22          1  2.4832 69.191  61.524
## 23          1  3.5369 60.654  61.524
## 24          1  2.0186 66.894  61.524
## 25          1  1.3804 25.514  61.524
## 26          1  1.4491 40.780  61.524
## 27          1  3.6367 45.574  61.524
## 28          1  2.4403 72.819  61.524
## 29          1  2.9025 93.024  61.524
## 30          1  4.1865 83.562  61.524
## 31          1  1.2935 49.999  61.524
## 32          1  1.4878 30.565  61.524
## 33          1  1.7071 65.081  61.524
## 34          1  3.4424 57.092  61.524
## 35          1  2.2499 44.320  61.524
## 36          1  2.7564 69.402  61.524
## 37          1  6.5187 65.396  61.524
## 38          1  1.4606 77.194  61.524
## 39          1  1.4276 20.589  61.524
## 40          1  2.3629 36.181  61.524
## 41          1  1.7891 53.314  61.524
## 42          1  2.1984 67.737  61.524
## 43          1  2.6098 80.457  61.524
## 44          1  2.1903 82.420  61.524
## 45          1  2.8408 66.527  61.524
## 46          1  2.1357 44.444  61.524
## 47          1  1.9320 40.821  61.524
## 48          1  1.5838 70.127  61.524
## 49          1  2.6703 77.498  61.524
## 50          1  3.3289 88.945  61.524
## 51          1  2.3670 49.561  61.524
## 52          1  2.3727 72.838  61.524
## 53          1  2.5446 66.845  61.524
## 54          1 16.2545 78.604  61.524
## 55          1  5.3967 74.401  61.524
## 56          1  1.8872 37.689  61.524
## 57          1  1.7618 67.357  61.524
## 58          1  1.7553 49.895  61.524
## 59          1  2.7065 49.377  61.524
## 60          1  1.7785 50.155  61.524
## 61          1  2.1270 44.482  61.524
## 62          1  1.5877 24.241  61.524
## 63          1  2.0831 72.561  61.524
## 64          1  2.0860 78.721  61.524
## 65          1  3.0206 74.174  61.524
## 66          1  1.6887 83.020  61.524
## 67          1  1.8236 68.690  61.524
## 68          1  1.5703 61.161  61.524
## 69          1  2.7689 46.891  61.524
## 70          1  2.1113 43.250  61.524
## 71          1  2.2995 19.674  61.524
## 72          1  3.9448 78.757  61.524
## 73          1  1.7494 40.249  61.524
## 74          1  2.6960 66.409  61.524
## 75          1  5.0158 44.909  61.524
## 76          1  1.5877 65.049  61.524
## 77          1  1.5466 59.619  61.524
## 78          1  1.9674 45.276  61.524
## 79          1  3.0038 66.910  61.524
## 80          1  1.9589 58.267  61.524
## 81          1  2.0112 59.376  61.524
## 82          1  4.9696 60.841  61.524
## 83          1  1.4743 71.737  61.524
## 84          1  2.1745 64.742  61.524
## 85          1  3.8859 65.900  61.524
## 86          1  1.6447 71.756  61.524
## 87          1  5.8363 75.321  61.524
## 88          1  3.8642 70.661  61.524
## 89          1  2.1960 68.504  61.524
## 90          1  2.0462 75.661  61.524
## 91          1  3.3095 78.357  61.524
## 92          1  1.4572 69.462  61.524
## 93          1  1.9528 77.500  61.524
## 94          1  1.5099 60.695  61.524
## 95          1  2.4882 49.492  61.524
## 96          1  3.9164 75.294  61.524
## 97          1  1.5306 68.753  61.524
## 98          1  3.3930 33.591  61.524
## 99          1  1.8424 79.896  61.524
## 100         1  1.8241 79.592  61.524
## 101         1  1.9153 75.149  61.524
## 102         1  3.1790 69.383  61.524
## 103         1  3.3710 76.684  61.524
## 104         1  1.5214 75.866  61.524
## 105         1  1.5729 73.749  61.524
## 106         1  3.3615 61.774  61.524
## 107         1  2.7628 83.313  61.524
## 108         1  4.0029 51.573  61.524
## 109         1  2.4431 66.456  61.524
## 110         1  2.0937 21.927  61.524
## 111         1  1.6193 46.519  61.524
## 112         1  2.6827 68.000  61.524
## 113         1  2.4447 79.691  61.524
## 114         1  2.9151 69.246  61.524
## 115         1  1.6740 51.636  61.524
## 116         1  2.1313 54.886  61.524
## 117         1  5.3241 49.451  61.524
## 118         1  2.4790 72.704  61.524
## 119         1  2.1147 59.885  61.524
## 120         1  2.7789 24.479  61.524
## 121         1  1.7161 40.419  61.524
## 122         1  3.3860 68.200  61.524
## 123         1  1.9406 68.203  61.524
## 124         1  1.6071 58.836  61.524
## 125         1  1.8814 65.596  61.524
## 126         1  5.7953 61.424  61.524
## 127         1  2.3158 76.580  61.524
## 128         1  2.0933 42.248  61.524
## 129         1  5.1587 69.125  61.524
## 130         1  1.5596 63.244  61.524
## 131         1  6.9089 31.099  61.524
## 132         1  1.7792 74.787  61.524
## 133         1  1.9104 66.146  61.524
## 134         1  2.8644 72.682  61.524
## 135         1  1.4875 36.674  61.524
## 136         1  1.9032 41.265  61.524
## 137         1  6.2045 77.536  61.524
## 138         1  1.8540 61.919  61.524
## 139         1  6.6726 76.249  61.524
## 140         1  2.4113 71.641  61.524
## 141         1  2.0096 57.747  61.524
## 142         1  3.6856 69.202  61.524
## 143         1  1.8835 47.932  61.524
## 144         1  2.5738 45.268  61.524
## 145         1  1.4267 72.550  61.524
## 146         1  1.4684 69.706  61.524
## 147         1  1.5509 44.895  61.524
## 148         1  1.6841 61.733  61.524
## 149         1  2.4244 50.576  61.524
## 150         1  2.7876 64.704  61.524
## 151         1  1.4540 57.355  61.524
## 152         1  1.6034 66.721  61.524
## 153         1  2.9338 58.598  61.524
## 154         1  1.2151 56.824  61.524
## 155         1  1.6747 82.582  61.524
## 156         1  1.8670 85.451  61.524
## 157         1  1.3442 32.427  61.524
## 158         1  1.4639 75.458  61.524
## 159         1  1.9305 51.014  61.524
## 160         1  1.9003 50.886  61.524
## 161         1  2.7129 55.047  61.524
## 162         1  1.5933 67.012  61.524
## 163         1  4.7687 84.758  61.524
## 164         1  2.1449 46.133  61.524
## 165         1  2.0824 34.144  61.524
## 166         1  1.4556 38.765  61.524
## 167         1  2.2810 75.907  61.524
## 168         1  2.1964 76.780  61.524
## 169         1  2.3683 64.931  61.524
## 170         1  1.7662 41.522  61.524
## 171         1  1.5903 31.797  61.524
## 172         1  2.6648 74.023  61.524
## 173         1  2.0869 48.591  61.524
## 174         1  2.5654 58.491  61.524
## 175         1  1.5336 83.978  61.524
## 176         1  1.8909 60.813  61.524
## 177         1  5.2376 75.680  61.524
## 178         1  1.5439 62.858  61.524
## 179         1  2.2578 84.665  61.524
## 180         1  5.2620 31.283  61.524
## 181         1  3.9943 67.170  61.524
## 182         1  4.6442 71.663  61.524
## 183         1  1.5067 36.463  61.524
## 184         1  3.0795 46.155  61.524
## 185         1  1.3850 62.053  61.524
## 186         1  2.6487 78.820  61.524
## 187         1  1.4324 59.351  61.524
## 188         1  2.2382 49.415  61.524
## 189         1  6.7059 75.107  61.524
## 190         1  4.2827 55.047  61.524
## 191         1  1.6035 42.957  61.524
## 192         1  3.9724 80.219  61.524
## 193         1  1.7799 36.903  61.524
## 194         1  3.5915 62.853  61.524
## 195         1  1.7450 45.407  61.524
## 196         1  1.7735 41.645  61.524
## 197         1  2.0620 72.660  61.524
## 198         1  2.8653 72.110  61.524
## 199         1  5.8884 74.990  61.524
## 200         1  4.0935 71.261  61.524
## 201         1  2.2277 62.344  61.524
## 202         1  1.4621 30.015  61.524
## 203         1  4.2709 61.687  61.524
## 204         1  2.3274 42.136  61.524
## 205         1  2.9263 42.834  61.524
## 206         1  2.1012 50.951  61.524
## 207         1  2.2742 70.927  61.524
## 208         1  2.6215 30.834  61.524
## 209         1  2.0419 69.777  61.524
## 210         1  1.3876 71.042  61.524
## 211         1  1.9934 46.801  61.524
## 212         1  2.4158 77.859  61.524
## 213         1  1.6774 69.216  61.524
## 214         1  1.4983 27.107  61.524
## 215         1  3.4275 74.648  61.524
## 216         1  1.9552 72.331  61.524
## 217         1  3.0966 55.688  61.524
## 218         1  1.8612 59.740  61.524
## 219         1  1.8128 74.524  61.524
## 220         1  1.6292 87.192  61.524
## 221         1  1.7425 74.439  61.524
## 222         1  2.3432 54.982  61.524
## 223         1  3.3799 45.372  61.524
## 224         1  1.3641 80.444  61.524
## 225         1  2.0511 63.978  61.524
## 226         1  2.5923 65.624  61.524
## 227         1  2.1958 68.441  61.524
## 228         1  1.4176 74.297  61.524
## 229         1  1.4323 74.754  61.524
## 230         1  1.7438 77.060  61.524
## 231         1  3.3300 77.177  61.524
## 232         1  1.6880 65.577  61.524
## 233         1  1.3210 37.251  61.524
## 234         1  2.3043 69.133  61.524
## 235         1  1.8100 61.372  61.524
## 236         1  1.6601 75.244  61.524
## 237         1  1.6425 70.437  61.524
## 238         1  1.9681 50.894  61.524
## 239         1  2.0674 59.398  61.524
## 240         1  3.6702 78.869  61.524
## 241         1  1.7965 71.064  61.524
## 242         1  1.3612 68.969  61.524
## 243         1  6.0535 84.416  61.524
## 244         1  1.7622 38.804  61.524
## 245         1  2.6025 48.638  61.524
## 246         1  4.1011 75.877  61.524
## 247         1  1.7027 58.954  61.524
## 248         1  2.3517 56.920  61.524
## 249         1  1.6146 58.204  61.524
## 250         1  1.8816 39.392  61.524
##  [ reached 'max' / getOption("max.print") -- omitted 1934 rows ]
```
]

---
class: middle

## Causal relative risk 
  
.small[

```r
# get causal relative risk, use weighted GLM 
glm.obj &lt;- glm(died ~ treatment, weights = weight, 
               data = dat, family = binomial(link = log))
# summary(glm.obj)
betaiptw &lt;- coef(glm.obj)
# to properly account for weighting, use asymptotic (sandwich) variance

SE &lt;- sqrt(diag(vcovHC(glm.obj, type = "HC0")))

# Get the point estimate and confidence intervals
causalrr &lt;- exp(betaiptw[2])
lrr &lt;- exp(betaiptw[2] - 1.96*SE[2])
urr &lt;- exp(betaiptw[2] + 1.96*SE[2])
c(causalrr, lrr, urr)
```

```
## treatment treatment treatment 
##    1.0455    1.0017    1.0911
```
]

???
because the weights in the `glm` function will make the sample size bigger than it actually was, so robust/sandwich covariance can help us to correct that. 

`vcovHC` gives us robust variance-covariance matrix

`diag` keep the numbers in the diagonal 

then take square root to calculate the standard error.

---
class: middle

## Causal risk difference 

.small[

```r
# get causal risk difference, use weighted GLM 
glm.obj &lt;- glm(died ~ treatment, weights = weight, 
               data = dat, family = binomial(link = "identity"))
# summary(glm.obj)
betaiptw &lt;- coef(glm.obj)
SE &lt;- sqrt(diag(vcovHC(glm.obj, type = "HC0")))

# causal risk difference
causalrd &lt;- betaiptw[2]
lrd &lt;- betaiptw[2]  - 1.96*SE[2]
urd &lt;- betaiptw[2]  + 1.96*SE[2]
c(causalrd, lrd, urd)
```

```
##  treatment  treatment  treatment 
## 0.02911995 0.00094955 0.05729035
```
]

---
class: middle

## Let's repeat using the `ipw` package

.small[

```r
weightmodel &lt;- ipwpoint(exposure = treatment, 
                        family = "binomial",
                        link = "logit",
                        denominator = ~ age + female + meanbp1 + ARF +
                          CHF + Cirr + colcan + Coma + lungcan + MOSF + 
                          sepsis + aps, data = dat)

summary(weightmodel$ipw.weights)
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.05    1.37    1.68    1.99    2.24   17.30
```
]


.small[

```r
# get the density plot easily
ipwplot(weights = weightmodel$ipw.weights, logscale = FALSE,
        main = "Weights", xlim = c(0, 18))
```

]

---
class: inverse
background-image: url("./fig/densityplotWeights.png")
background-position: 50% 50%
background-size: contain

???
this figure is showing the distribution of the weights. 


---
class: middle

## Get causal risk difference 

.small[

```r
# causal risk difference using survey designed glm
svyglm.obj &lt;- (svyglm(died ~ treatment, 
                      design = svydesign(~ 1, weights = ~weight, data = dat)))

coef(svyglm.obj)
```

```
## (Intercept)   treatment 
##     0.64052     0.02912
```

```r
confint(svyglm.obj)
```

```
##                  2.5 %   97.5 %
## (Intercept) 0.62405145 0.656986
## treatment   0.00094761 0.057292
```
]

---
class: middle

## Truncated weights 

.small[
```r
weight &lt;- dat$weight
truncweight &lt;- replace(weight, weight &gt; 15, 15)
# causal risk difference
glm.obj &lt;- glm(died ~ treatment, weights = truncweight, 
               family = binomial(link = "identity"), 
               data = dat)

# truncated weights use ipw package
weightmodel &lt;- ipwpoint(exposure = treatment, family = "binomial",
                        link = "logit", 
                        denominator = ~ age + female + meanbp1 + ARF +
                          CHF + Cirr + colcan + Coma + lungcan + MOSF + 
                          sepsis + aps, data = dat, 
*                       trunc = 0.01)

```
]

---
class: middle

## Truncate by percentile

.small[

]

.small[

```r
summary(weightmodel$weights.trun)
```

```
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.08    1.37    1.68    1.97    2.24    6.34
```
]


.small[

```r
# get risk difference
dat$wt &lt;- weightmodel$weights.trun
svyglm.obj &lt;- (svyglm(died ~ treatment, 
                      design = svydesign(~ 1, weights = ~wt, data = dat)))
coef(svyglm.obj)
```

```
## (Intercept)   treatment 
##    0.640382    0.031196
```

```r
confint(svyglm.obj)
```

```
##                 2.5 %   97.5 %
## (Intercept) 0.6239219 0.656842
## treatment   0.0040138 0.058379
```
]

---
class: inverse, bottom
background-image: url("./fig/truncatedWeights.png")
background-position: 50% 50%
background-size: contain


.small[

]


---
class: inverse, center, middle

# Test for unobserved covariates

---
class: middle

## Example 1 (Correct model)

.small[

```r
psmodel &lt;- glm(treatment ~ age + female + meanbp1 + ARF + 
                 CHF + Cirr + colcan + Coma + lungcan + 
                 MOSF + sepsis + aps, family = binomial(link = "logit"), 
               data = dat)

ps &lt;- predict(psmodel, type = "response")

dat["residuals"] &lt;- residuals.glm(psmodel)

dat &lt;- dat %&gt;%
  mutate(id_num = 1:n())

m.out &lt;- MatchIt::matchit(treatment ~ age + female + meanbp1 + ARF + 
                 CHF + Cirr + colcan + Coma + lungcan + 
                 MOSF + sepsis + aps, data = dat, 
                 method = "nearest", distance = "logit", caliper = 0.2)
```
]


---
class: middle

## Manipulation

.small[

```r
a &lt;- data.frame(dat$id_num, m.out$treat, m.out$weights)
colnames(a) &lt;- c("id_num", "trt", "weights")

# datafile of matches
b &lt;- as.data.frame(m.out$match.matrix)
colnames(b) &lt;- c("matched_unit")
b$matched_unit &lt;- as.numeric(as.character(b$matched_unit))
b$treated_unit &lt;- as.numeric(rownames(b))
# now delete matches=na
c &lt;- b[!is.na(b$matched_unit), ]
c$match_num &lt;- 1:dim(c)[1]

# attach match number to large datafile
a[c$matched_unit, 4] &lt;- c$match_num
a[c$treated_unit, 4] &lt;- c$match_num
colnames(a)[4] &lt;- "match_num"

final_dat &lt;- dat %&gt;%
  dplyr::select(-id_num) %&gt;%
  cbind(a) %&gt;%
  filter(!(is.na(a$match_num)))
```
]


---
class: middle

## Hausman test

.small[

```r
# Hausman test

final_dat &lt;- final_dat %&gt;%
  dplyr::mutate(dead = if_else(death == "No", 0, 1))

outcomemodel &lt;- survival::clogit(dead ~ age + female + meanbp1 + ARF +
  CHF + Cirr + Coma + lungcan +
  MOSF + sepsis + aps + strata(match_num) + residuals,
data = final_dat)

covariance &lt;- diag(vcov(outcomemodel))
covariance &lt;- covariance[length(covariance)]

coefficient &lt;- coef(outcomemodel)[length(coef(outcomemodel))]

z_stat &lt;- coefficient / sqrt(covariance)
p_values &lt;- pchisq(z_stat^2, 1, lower.tail = FALSE)
names(p_values)=NULL
cat(paste0("P value: ",round(p_values,digit=4)))
```
]

.small[
```
# P value: 0.8558
```
]

???
We cannot reject the null hypothesis, and hence we cannot invalidate the absence of confounders.


---
class: middle 

## Example 2 (Residual confounders - Extreme example)

.small[


```r
#Propensity score
psmodel &lt;- glm(treatment ~ age,
                  family = binomial(link = "logit"),
                  data = dat)

ps &lt;- predict(psmodel, type = "response")

dat["residuals"] &lt;- residuals.glm(psmodel)

dat &lt;- dat %&gt;%
  mutate(id_num = 1:n())

m.out &lt;- MatchIt::matchit(treatment ~ age,
data = dat, method = "nearest", distance = "logit", caliper = 0.2
)
```
]


---
class: middle

## Manipulation

.small[

```r
a &lt;- data.frame(dat$id_num, m.out$treat, m.out$weights)
colnames(a) &lt;- c("id_num", "trt", "weights")

# datafile of matches
b &lt;- as.data.frame(m.out$match.matrix)
colnames(b) &lt;- c("matched_unit")
b$matched_unit &lt;- as.numeric(as.character(b$matched_unit))
b$treated_unit &lt;- as.numeric(rownames(b))
# now delete matches=na
c &lt;- b[!is.na(b$matched_unit), ]
c$match_num &lt;- 1:dim(c)[1]

# attach match number to large datafile
a[c$matched_unit, 4] &lt;- c$match_num
a[c$treated_unit, 4] &lt;- c$match_num
colnames(a)[4] &lt;- "match_num"

final_dat &lt;- dat %&gt;%
  dplyr::select(-id_num) %&gt;%
  cbind(a) %&gt;%
  filter(!(is.na(a$match_num)))
```
]

---
class: middle

## Hausman test

.small[

```r
# Hausman test
library(survival)
final_dat &lt;- final_dat %&gt;%
  mutate(dead = ifelse(death == "No", 0, 1))

outcomemodel &lt;- survival::clogit(dead ~ age + strata(match_num) + residuals,
data = final_dat
)

covariance &lt;- diag(vcov(outcomemodel))
covariance &lt;- covariance[length(covariance)]

coefficient &lt;- coef(outcomemodel)[length(coef(outcomemodel))]

z_stat &lt;- coefficient / sqrt(covariance)
p_values &lt;- pchisq(z_stat^2, 1, lower.tail = FALSE)
names(p_values)=NULL

cat(paste0("P value: ",formatC(p_values,format="e",digits=2)))
```
]

.small[
```
# P value: 1.93e-02
```
]


???
In this case, we reject the null hypothesis `\(H_0:\delta=0\)`, and conclude about the presence of confounders.

---
class: middle 

# Reading list

1. 5 misunderstandings about propensity score &lt;br&gt; [PSに関する5つの誤解](https://healthpolicyhealthecon.com/2015/05/07/propensity-score-2/)

2. Causal inference in Statistics,  [統計学における因果推論（ルービンの因果モデル）](https://healthpolicyhealthecon.com/2014/11/30/rubin_causal_model/)

3. Tableone package has many useful code for tables and figures: [https://cran.r-project.org/web/packages/tableone/vignettes/smd.html](https://cran.r-project.org/web/packages/tableone/vignettes/smd.html)

4. `rcbalance` package has many more matching options: [https://obsstudies.org/wp-content/uploads/2017/06/rcbalance_paper_v7r2.pdf](https://obsstudies.org/wp-content/uploads/2017/06/rcbalance_paper_v7r2.pdf)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function(time) {
  var d2 = function(number) {
    return ('0' + number).slice(-2); // left-pad 0 to minutes/seconds
  },

  time_format = function(total) {
    var secs = Math.abs(total) / 1000;
    var h = Math.floor(secs / 3600);
    var m = Math.floor(secs % 3600 / 60);
    var s = Math.round(secs % 60);
    var res = d2(m) + ':' + d2(s);
    if (h > 0) res = h + ':' + res;
    return res;  // [hh:]mm:ss
  },

  slide_number_div = function(i) {
    return document.getElementsByClassName('remark-slide-number').item(i);
  },

  current_page_number = function(i) {
    return slide_number_div(i).firstChild.textContent;  // text "i / N"
  };

  var timer = document.createElement('span'); timer.id = 'slide-time-left';
  var time_left = time, k = slideshow.getCurrentSlideIndex(),
      last_page_number = current_page_number(k);

  setInterval(function() {
    time_left = time_left - 1000;
    timer.innerHTML = ' ' + time_format(time_left);
    if (time_left < 0) timer.style.color = 'red';
  }, 1000);

  slide_number_div(k).appendChild(timer);

  slideshow.on('showSlide', function(slide) {
    var i = slide.getSlideIndex(), n = current_page_number(i);
    // reset timer when a new slide is shown and the page number is changed
    if (last_page_number !== n) {
      time_left = time; last_page_number = n;
      timer.innerHTML = ' ' + time_format(time); timer.style.color = null;
    }
    slide_number_div(i).appendChild(timer);
  });
})(60000);
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
